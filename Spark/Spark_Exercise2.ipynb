{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "buried-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "catholic-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile = sc.textFile(\"notebook.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "prescribed-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the lines that contains INFO\n",
    "linesWithINFO = logFile.filter(lambda line: \"INFO\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "expensive-differential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the lines with INFO\n",
    "logFile.filter(lambda line: \"INFO\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "romantic-production",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the lines with INFO\n",
    "linesWithINFO.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "prescription-slovenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count no of lines with spark within the lines of INFO\n",
    "linesWithINFO.filter(lambda line: \"spark\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "resistant-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count no of lines with spark in logFile\n",
    "logFile.filter(lambda line: \"spark\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "historical-story",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15/10/14 14:29:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53333]',\n",
       " \"15/10/14 14:29:23 INFO Utils: Successfully started service 'sparkDriver' on port 53333.\",\n",
       " '15/10/14 14:29:23 INFO DiskBlockManager: Created local directory at /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a',\n",
       " '15/10/14 14:29:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/httpd-ed3f4ab0-7218-48bc-9d8a-3981b1cfe574',\n",
       " \"15/10/14 14:29:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35726.\",\n",
       " '15/10/15 15:33:42 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:47412]',\n",
       " \"15/10/15 15:33:42 INFO Utils: Successfully started service 'sparkDriver' on port 47412.\",\n",
       " '15/10/15 15:33:42 INFO DiskBlockManager: Created local directory at /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261',\n",
       " '15/10/15 15:33:42 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/httpd-80730048-1dcb-4da2-8458-8bf3eba96046',\n",
       " \"15/10/15 15:33:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47915.\",\n",
       " '15/10/16 13:08:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58378]',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'sparkDriver' on port 58378.\",\n",
       " '15/10/16 13:08:23 INFO DiskBlockManager: Created local directory at /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0',\n",
       " '15/10/16 13:08:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/httpd-98632027-ee06-401b-a027-b7973b158023',\n",
       " \"15/10/16 13:08:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34420.\",\n",
       " '15/10/16 13:13:22 INFO Utils: path = /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0, already present as root for deletion.',\n",
       " '15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/pyspark-2107622a-f8ad-4b5a-b456-f0e414fbed40',\n",
       " '15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70',\n",
       " '15/10/16 13:13:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:38668]',\n",
       " \"15/10/16 13:13:27 INFO Utils: Successfully started service 'sparkDriver' on port 38668.\",\n",
       " '15/10/16 13:13:27 INFO DiskBlockManager: Created local directory at /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/blockmgr-e0992345-e860-44ea-aaca-50e75bd99684',\n",
       " '15/10/16 13:13:27 INFO HttpFileServer: HTTP File server directory is /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/httpd-fe5e1d28-9663-460b-97fd-e2374b912583',\n",
       " \"15/10/16 13:13:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44407.\",\n",
       " '15/10/16 14:52:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43750]',\n",
       " \"15/10/16 14:52:20 INFO Utils: Successfully started service 'sparkDriver' on port 43750.\",\n",
       " '15/10/16 14:52:20 INFO DiskBlockManager: Created local directory at /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221',\n",
       " '15/10/16 14:52:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/httpd-a9ac31c5-fdd1-4437-a29f-771847924c71',\n",
       " \"15/10/16 14:52:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54796.\",\n",
       " '15/10/21 06:09:21 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43928]',\n",
       " \"15/10/21 06:09:21 INFO Utils: Successfully started service 'sparkDriver' on port 43928.\",\n",
       " '15/10/21 06:09:21 INFO DiskBlockManager: Created local directory at /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016',\n",
       " '15/10/21 06:09:21 INFO HttpFileServer: HTTP File server directory is /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/httpd-3f3cd3ee-81f2-4ba5-be62-ccdb6d62cf52',\n",
       " \"15/10/21 06:09:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34705.\",\n",
       " '15/10/21 06:18:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34767]',\n",
       " \"15/10/21 06:18:20 INFO Utils: Successfully started service 'sparkDriver' on port 34767.\",\n",
       " '15/10/21 06:18:20 INFO DiskBlockManager: Created local directory at /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555',\n",
       " '15/10/21 06:18:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/httpd-81687cf4-f5a6-4a97-8e52-a6096ad60235',\n",
       " \"15/10/21 06:18:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58989.\",\n",
       " '15/10/21 06:44:27 INFO Utils: path = /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261, already present as root for deletion.',\n",
       " '15/10/21 06:44:27 INFO Utils: Deleting directory /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46',\n",
       " '15/10/21 06:44:41 INFO Utils: path = /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016, already present as root for deletion.',\n",
       " '15/10/21 06:44:42 INFO Utils: Deleting directory /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a',\n",
       " '15/10/21 06:46:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44681]',\n",
       " \"15/10/21 06:46:03 INFO Utils: Successfully started service 'sparkDriver' on port 44681.\",\n",
       " '15/10/21 06:46:03 INFO DiskBlockManager: Created local directory at /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62',\n",
       " '15/10/21 06:46:04 INFO HttpFileServer: HTTP File server directory is /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/httpd-ca2b9527-9689-44df-90b9-94eb76bf22c8',\n",
       " \"15/10/21 06:46:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33807.\",\n",
       " '15/10/21 06:46:06 INFO Utils: path = /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62, already present as root for deletion.',\n",
       " '15/10/21 06:46:06 INFO Utils: Deleting directory /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5',\n",
       " '15/10/21 06:46:18 INFO Utils: path = /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555, already present as root for deletion.',\n",
       " '15/10/21 06:46:19 INFO Utils: Deleting directory /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350',\n",
       " '15/10/21 06:46:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34749]',\n",
       " \"15/10/21 06:46:20 INFO Utils: Successfully started service 'sparkDriver' on port 34749.\",\n",
       " '15/10/21 06:46:20 INFO DiskBlockManager: Created local directory at /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f',\n",
       " '15/10/21 06:46:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/httpd-72d3e35d-a6b4-427b-b7cd-7f40b45041ae',\n",
       " \"15/10/21 06:46:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43363.\",\n",
       " '15/10/21 06:51:44 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58291]',\n",
       " \"15/10/21 06:51:44 INFO Utils: Successfully started service 'sparkDriver' on port 58291.\",\n",
       " '15/10/21 06:51:44 INFO DiskBlockManager: Created local directory at /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115',\n",
       " '15/10/21 06:51:44 INFO HttpFileServer: HTTP File server directory is /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/httpd-c3af5d8f-93b1-4ea1-b4cd-d939821a87ee',\n",
       " \"15/10/21 06:51:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60134.\",\n",
       " '15/10/21 06:53:15 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:52949]',\n",
       " \"15/10/21 06:53:15 INFO Utils: Successfully started service 'sparkDriver' on port 52949.\",\n",
       " '15/10/21 06:53:15 INFO DiskBlockManager: Created local directory at /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/blockmgr-33399bc4-6281-42c6-b023-b7bcd4a56bc2',\n",
       " '15/10/21 06:53:15 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/httpd-0637bf42-85e3-45a9-a395-9fadcb6744a4',\n",
       " \"15/10/21 06:53:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42148.\",\n",
       " '15/10/21 06:53:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:48625]',\n",
       " \"15/10/21 06:53:37 INFO Utils: Successfully started service 'sparkDriver' on port 48625.\",\n",
       " '15/10/21 06:53:37 INFO DiskBlockManager: Created local directory at /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a',\n",
       " '15/10/21 06:53:37 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/httpd-034f2b34-e002-40b1-9500-9409076170ec',\n",
       " \"15/10/21 06:53:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53086.\",\n",
       " '15/10/21 06:54:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34793]',\n",
       " \"15/10/21 06:54:55 INFO Utils: Successfully started service 'sparkDriver' on port 34793.\",\n",
       " '15/10/21 06:54:55 INFO DiskBlockManager: Created local directory at /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654',\n",
       " '15/10/21 06:54:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/httpd-7dd197ab-d78a-45df-a99f-0cdd16edd456',\n",
       " \"15/10/21 06:54:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50977.\",\n",
       " '15/10/21 06:54:59 INFO Utils: path = /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654, already present as root for deletion.',\n",
       " '15/10/21 06:55:00 INFO Utils: Deleting directory /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb',\n",
       " '15/10/21 06:55:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53265]',\n",
       " \"15/10/21 06:55:20 INFO Utils: Successfully started service 'sparkDriver' on port 53265.\",\n",
       " '15/10/21 06:55:20 INFO DiskBlockManager: Created local directory at /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e',\n",
       " '15/10/21 06:55:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/httpd-a749eee4-3bc1-429a-94d0-d221f2f3738a',\n",
       " \"15/10/21 06:55:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45478.\",\n",
       " '15/10/21 06:55:22 INFO Utils: path = /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e, already present as root for deletion.',\n",
       " '15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/pyspark-6db0c8fd-094a-4f08-bd68-dff219e65350',\n",
       " '15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a',\n",
       " '15/10/21 07:14:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:45827]',\n",
       " \"15/10/21 07:14:56 INFO Utils: Successfully started service 'sparkDriver' on port 45827.\",\n",
       " '15/10/21 07:14:56 INFO DiskBlockManager: Created local directory at /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/blockmgr-573c0859-eb51-466c-99b6-84c3311f512c',\n",
       " '15/10/21 07:14:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/httpd-4a3e389d-7784-4587-95d4-46cd1d001fca',\n",
       " \"15/10/21 07:14:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53959.\",\n",
       " '15/10/21 07:56:30 [INFO] Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:39281]',\n",
       " '15/10/21 15:43:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:39173]',\n",
       " \"15/10/21 15:43:57 INFO Utils: Successfully started service 'sparkDriver' on port 39173.\",\n",
       " '15/10/21 15:43:57 INFO DiskBlockManager: Created local directory at /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/blockmgr-586c41e5-996a-4a68-87a5-5b736a9618b6',\n",
       " '15/10/21 15:43:57 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/httpd-4b254c9b-ffb8-4603-bf57-49661e22248d',\n",
       " \"15/10/21 15:43:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54908.\",\n",
       " '15/10/21 17:02:06 INFO Utils: path = /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/blockmgr-586c41e5-996a-4a68-87a5-5b736a9618b6, already present as root for deletion.',\n",
       " '15/10/21 17:02:07 INFO Utils: Deleting directory /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8',\n",
       " '15/10/21 17:02:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:42749]',\n",
       " \"15/10/21 17:02:12 INFO Utils: Successfully started service 'sparkDriver' on port 42749.\",\n",
       " '15/10/21 17:02:12 INFO DiskBlockManager: Created local directory at /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/blockmgr-264e8036-5ed0-4b03-b896-6ff04e27f572',\n",
       " '15/10/21 17:02:12 INFO HttpFileServer: HTTP File server directory is /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/httpd-fc7a9c70-76bc-4605-958f-e115bd3e8d47',\n",
       " \"15/10/21 17:02:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45162.\",\n",
       " '15/10/22 02:32:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44439]',\n",
       " \"15/10/22 02:32:26 INFO Utils: Successfully started service 'sparkDriver' on port 44439.\",\n",
       " '15/10/22 02:32:26 INFO DiskBlockManager: Created local directory at /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/blockmgr-ff3b1fac-22e5-4969-8e3a-2aecbf2c0dcc',\n",
       " '15/10/22 02:32:26 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/httpd-af60c2aa-69b8-4878-86e3-43b8fccdb6ac',\n",
       " \"15/10/22 02:32:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59251.\",\n",
       " '15/10/22 04:36:32 INFO Utils: path = /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/blockmgr-264e8036-5ed0-4b03-b896-6ff04e27f572, already present as root for deletion.',\n",
       " '15/10/22 04:36:32 INFO Utils: Deleting directory /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500',\n",
       " '15/10/22 04:36:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:57259]',\n",
       " \"15/10/22 04:36:37 INFO Utils: Successfully started service 'sparkDriver' on port 57259.\",\n",
       " '15/10/22 04:36:37 INFO DiskBlockManager: Created local directory at /tmp/spark-6ffd920e-2dd5-43d4-a2b8-6b3c3a1ae0c7/blockmgr-f28c1757-42ed-4495-bca7-f4693f2f1846',\n",
       " '15/10/22 04:36:38 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6ffd920e-2dd5-43d4-a2b8-6b3c3a1ae0c7/httpd-8bda8744-f6f5-481c-9dd9-065b7bf0f7b9',\n",
       " \"15/10/22 04:36:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34923.\",\n",
       " '15/10/22 05:42:05 INFO Utils: path = /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221, already present as root for deletion.',\n",
       " '15/10/22 05:42:06 INFO Utils: Deleting directory /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4',\n",
       " '15/10/22 05:42:59 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:32997]',\n",
       " \"15/10/22 05:42:59 INFO Utils: Successfully started service 'sparkDriver' on port 32997.\",\n",
       " '15/10/22 05:42:59 INFO DiskBlockManager: Created local directory at /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/blockmgr-6da140df-5530-460e-a154-b780fb3839ff',\n",
       " '15/10/22 05:43:00 INFO HttpFileServer: HTTP File server directory is /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/httpd-5a4d1c53-dd5f-4d13-8f82-fb56ecc67896',\n",
       " \"15/10/22 05:43:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39140.\",\n",
       " '15/10/22 05:44:02 INFO Utils: path = /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/blockmgr-6da140df-5530-460e-a154-b780fb3839ff, already present as root for deletion.',\n",
       " '15/10/22 05:44:03 INFO Utils: Deleting directory /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59',\n",
       " '15/10/22 05:44:23 INFO Utils: path = /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a, already present as root for deletion.',\n",
       " '15/10/22 05:44:24 INFO Utils: Deleting directory /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3',\n",
       " '15/10/22 05:44:31 INFO Utils: path = /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f, already present as root for deletion.',\n",
       " '15/10/22 05:44:32 INFO Utils: Deleting directory /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/pyspark-9e9ff3f6-2e32-4570-ae7a-22a651278319',\n",
       " '15/10/22 05:44:32 INFO Utils: Deleting directory /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f',\n",
       " '15/10/22 05:44:42 INFO Utils: path = /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/blockmgr-ff3b1fac-22e5-4969-8e3a-2aecbf2c0dcc, already present as root for deletion.',\n",
       " '15/10/22 05:44:43 INFO Utils: Deleting directory /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/pyspark-00ef6c66-4db7-4741-b890-7647fc2d4f76',\n",
       " '15/10/22 05:44:43 INFO Utils: Deleting directory /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213',\n",
       " '15/10/22 05:44:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:46937]',\n",
       " \"15/10/22 05:44:57 INFO Utils: Successfully started service 'sparkDriver' on port 46937.\",\n",
       " '15/10/22 05:44:57 INFO DiskBlockManager: Created local directory at /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/blockmgr-e7a8bf00-0701-4cb7-a835-b8448d9fe79c',\n",
       " '15/10/22 05:44:57 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/httpd-91230dd5-3f29-4655-906e-228bc7bde472',\n",
       " \"15/10/22 05:44:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37166.\",\n",
       " '15/10/22 05:45:01 INFO Utils: path = /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/blockmgr-e7a8bf00-0701-4cb7-a835-b8448d9fe79c, already present as root for deletion.',\n",
       " '15/10/22 05:45:02 INFO Utils: Deleting directory /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/pyspark-c5a99eda-9137-401b-99a1-ffeae801f695',\n",
       " '15/10/22 05:45:02 INFO Utils: Deleting directory /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f',\n",
       " '15/10/22 05:48:07 INFO Utils: path = /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115, already present as root for deletion.',\n",
       " '15/10/22 05:48:08 INFO Utils: Deleting directory /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5',\n",
       " '15/10/22 06:12:32 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:56255]',\n",
       " \"15/10/22 06:12:32 INFO Utils: Successfully started service 'sparkDriver' on port 56255.\",\n",
       " '15/10/22 06:12:32 INFO DiskBlockManager: Created local directory at /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/blockmgr-88b233e4-cd52-4810-b90f-fd20425e41c4',\n",
       " '15/10/22 06:12:32 INFO HttpFileServer: HTTP File server directory is /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/httpd-0f8a2ab5-bb96-4597-98e9-db0d62952c1f',\n",
       " \"15/10/22 06:12:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34717.\",\n",
       " '15/10/22 06:33:24 INFO Utils: path = /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/blockmgr-88b233e4-cd52-4810-b90f-fd20425e41c4, already present as root for deletion.',\n",
       " '15/10/22 06:33:25 INFO Utils: Deleting directory /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/pyspark-7fef74de-62da-4241-a21b-35d6b6075968',\n",
       " '15/10/22 06:33:25 INFO Utils: Deleting directory /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2',\n",
       " '15/10/22 06:33:30 INFO Utils: path = /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a, already present as root for deletion.',\n",
       " '15/10/22 06:33:30 INFO Utils: Deleting directory /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6',\n",
       " '15/10/22 06:33:35 INFO Utils: path = /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/blockmgr-573c0859-eb51-466c-99b6-84c3311f512c, already present as root for deletion.',\n",
       " '15/10/22 06:33:35 INFO Utils: Deleting directory /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/pyspark-56ef90f8-2b4f-43b5-a7e4-219c544e948e',\n",
       " '15/10/22 06:33:35 INFO Utils: Deleting directory /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch the lines(INFO + spark) as an array of Strings\n",
    "linesWithINFO.filter(lambda line: \"spark\" in line).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "authorized-occasions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[127] at RDD at PythonRDD.scala:53 []\\n |  notebook.log MapPartitionsRDD[120] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  notebook.log HadoopRDD[119] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "#View the graph of an RDD\n",
    "print(linesWithINFO.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-response",
   "metadata": {},
   "source": [
    "#### Joining RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "liked-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create RDDs for the same README and the POM files\n",
    "readmeFile = sc.textFile(\"README.md\")\n",
    "pomFile = sc.textFile(\"pom.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adequate-hotel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Count Spark keywords are in each file\n",
    "print(readmeFile.filter(lambda line: \"Spark\" in line).count())\n",
    "print(pomFile.filter(lambda line: \"Spark\" in line).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "legislative-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCount on each RDD so that the results are (K,V) pairs of (word,count)\n",
    "readmeCount = readmeFile.                    \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).             \\\n",
    "    reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "pomCount = pomFile.                          \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).            \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "existing-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme Count\n",
      "\n",
      "[('', 43), ('Spark is a fast and general cluster computing system for Big Data. It provides', 1), ('and Spark Streaming for stream processing.', 1), ('guide, on the [project web page](http://spark.apache.org/documentation.html)', 1), ('## Building Spark', 1), ('Spark is built using [Apache Maven](http://maven.apache.org/).', 1), (' build/mvn -DskipTests clean package', 1), ('Try the following command, which should return 1000:', 1), (' scala> sc.parallelize(1 to 1000).count()', 1), ('## Interactive Python Shell', 1), (' ./bin/pyspark', 1), ('And run the following command, which should also return 1000:', 1), ('Spark also comes with several sample programs in the `examples` directory.', 1), ('To run one of them, use `./bin/run-example <class> [params]`. For example:', 1), (' ./bin/run-example SparkPi', 1), ('will run the Pi example locally.', 1), ('You can set the MASTER environment variable when running examples to submit', 1), ('examples to a cluster. This can be a mesos:// or spark:// URL,', 1), ('can also use an abbreviated class name if the class is in the `examples`', 1), ('package. For instance:', 1), (' MASTER=spark://host:7077 ./bin/run-example SparkPi', 1), ('Many of the example programs print usage help if no params are given.', 1), ('Testing first requires [building Spark](#building-spark). Once Spark is built, tests', 1), ('can be run using:', 1), ('[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1), ('## A Note About Hadoop Versions', 1), ('storage systems. Because the protocols have changed in different versions of', 1), ('Hadoop, you must build Spark against the same version that your cluster runs.', 1), ('[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), ('for detailed guidance on building for a particular distribution of Hadoop, including', 1), ('distribution.', 1), ('## Configuration', 1), ('Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)', 1), ('in the online documentation for an overview on how to configure Spark.', 1), ('# Apache Spark', 1), ('high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 1), ('supports general computation graphs for data analysis. It also supports a', 1), ('rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 1), ('MLlib for machine learning, GraphX for graph processing,', 1), ('<http://spark.apache.org/>', 1), ('## Online Documentation', 1), ('You can find the latest Spark documentation, including a programming', 1), ('and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1), ('This README file only contains basic setup instructions.', 1), ('To build Spark and its example programs, run:', 1), ('(You do not need to do this if you downloaded a pre-built package.)', 1), ('More detailed documentation is available from the project site, at', 1), ('[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1), ('## Interactive Scala Shell', 1), ('The easiest way to start using Spark is through the Scala shell:', 1), (' ./bin/spark-shell', 1), ('Alternatively, if you prefer Python, you can use the Python shell:', 1), (' >>> sc.parallelize(range(1000)).count()', 1), ('## Example Programs', 1), ('\"yarn\" to run on YARN, and \"local\" to run', 1), ('locally with one thread, or \"local[N]\" to run locally with N threads. You', 1), ('## Running Tests', 1), (' ./dev/run-tests', 1), ('Please see the guidance on how to', 1), ('Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 1), ('Please refer to the build documentation at', 1), ('building for particular Hive and Hive Thriftserver distributions. See also', 1), ('[\"Third Party Hadoop Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)', 1), ('for guidance on building a Spark application that works with a particular', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Readme Count\\n\")\n",
    "print(readmeCount.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "informal-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pom Count\n",
      "\n",
      "[('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', 1), ('  ~ Licensed to the Apache Software Foundation (ASF) under one or more', 1), ('  ~ contributor license agreements.  See the NOTICE file distributed with', 1), ('  ~ The ASF licenses this file to You under the Apache License, Version 2.0', 1), (' http://www.apache.org/licenses/LICENSE-2.0', 1), ('  ~ distributed under the License is distributed on an \"AS IS\" BASIS,', 1), ('  ~ limitations under the License.', 1), ('  -->', 1), ('', 841), ('<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">', 1), ('  <modelVersion>4.0.0</modelVersion>', 1), ('  <parent>', 1), (' <groupId>org.apache.spark</groupId>', 2), (' <artifactId>spark-parent_2.10</artifactId>', 1), (' <version>1.6.0-SNAPSHOT</version>', 1), ('  <properties>', 1), (' <sbt.project.name>examples</sbt.project.name>', 1), ('  </properties>', 1), ('  <packaging>jar</packaging>', 1), ('  <dependencies>', 1), (' <dependency>', 24), ('<version>${project.version}</version>', 11), (' </dependency>', 24), ('<artifactId>spark-streaming_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-bagel_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-hive_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-graphx_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>', 1), ('<exclusions>', 6), (' <artifactId>protobuf-java</artifactId>', 1), (' <!-- SPARK-4455 -->', 4), (' <groupId>org.apache.hbase</groupId>', 5), (' <artifactId>hbase-annotations</artifactId>', 4), (' <artifactId>jruby-complete</artifactId>', 1), ('<artifactId>hbase-protocol</artifactId>', 1), ('<artifactId>hbase-common</artifactId>', 1), (' <exclusion>', 1), ('  <artifactId>netty</artifactId>', 1), (' <artifactId>hadoop-core</artifactId>', 1), (' <artifactId>hadoop-mapreduce-client-core</artifactId>', 1), (' <artifactId>hadoop-annotations</artifactId>', 1), (' <artifactId>commons-math</artifactId>', 1), (' <groupId>com.sun.jersey</groupId>', 4), (' <artifactId>jersey-core</artifactId>', 2), (' <groupId>org.slf4j</groupId>', 1), (' <artifactId>slf4j-api</artifactId>', 1), (' <artifactId>commons-io</artifactId>', 1), ('<scope>test</scope>', 2), ('<artifactId>commons-math3</artifactId>', 1), ('<groupId>com.twitter</groupId>', 1), ('<groupId>org.scalacheck</groupId>', 1), ('<artifactId>cassandra-all</artifactId>', 1), ('<version>1.2.6</version>', 1), (' <groupId>com.googlecode.concurrentlinkedhashmap</groupId>', 1), (' <artifactId>commons-cli</artifactId>', 1), (' <groupId>commons-codec</groupId>', 1), (' <groupId>commons-lang</groupId>', 1), (' <artifactId>commons-lang</artifactId>', 1), (' <groupId>commons-logging</groupId>', 1), (' <artifactId>commons-logging</artifactId>', 1), (' <artifactId>netty</artifactId>', 1), (' <groupId>jline</groupId>', 1), (' <groupId>org.apache.cassandra.deps</groupId>', 1), (' <artifactId>avro</artifactId>', 1), ('<groupId>com.github.scopt</groupId>', 1), ('<artifactId>scopt_${scala.binary.version}</artifactId>', 1), ('<version>3.2.0</version>', 1), ('them to be provided.', 1), ('  </dependencies>', 1), ('  <build>', 1), (' <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>', 1), (' <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>', 1), ('<plugin>', 3), ('  <groupId>org.apache.maven.plugins</groupId>', 3), ('  <artifactId>maven-deploy-plugin</artifactId>', 1), (' <skip>true</skip>', 2), ('  </configuration>', 3), ('</plugin>', 3), ('  <artifactId>maven-shade-plugin</artifactId>', 1), (' <shadedArtifactAttached>false</shadedArtifactAttached>', 1), (' <outputFile>${project.build.directory}/scala-${scala.binary.version}/spark-examples-${project.version}-hadoop${hadoop.version}.jar</outputFile>', 1), (' <artifactSet>', 1), ('<includes>', 1), (' </artifactSet>', 1), ('<filter>', 1), ('  <artifact>*:*</artifact>', 1), (' <exclude>META-INF/*.DSA</exclude>', 1), (' <exclude>META-INF/*.RSA</exclude>', 1), ('  </excludes>', 1), ('</filter>', 1), (' </filters>', 1), ('<transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\" />', 1), ('</transformer>', 2), ('<transformer implementation=\"org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer\">', 1), ('  <resource>log4j.properties</resource>', 1), ('  </build>', 1), ('<dependencies>', 1), (' <artifactId>spark-streaming-kinesis-asl_${scala.binary.version}</artifactId>', 1), ('</dependencies>', 1), (' </profile>', 6), ('  <flume.deps.scope>provided</flume.deps.scope>', 1), ('  <hadoop.deps.scope>provided</hadoop.deps.scope>', 1), ('<id>hbase-provided</id>', 1), ('  <hbase.deps.scope>provided</hbase.deps.scope>', 1), ('<id>parquet-provided</id>', 1), ('  <parquet.deps.scope>provided</parquet.deps.scope>', 1), ('  </profiles>', 1), ('<!--', 1), ('  ~ this work for additional information regarding copyright ownership.', 1), ('  ~ (the \"License\"); you may not use this file except in compliance with', 1), ('  ~ the License.  You may obtain a copy of the License at', 1), ('  ~', 3), ('  ~ Unless required by applicable law or agreed to in writing, software', 1), ('  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.', 1), ('  ~ See the License for the specific language governing permissions and', 1), (' <relativePath>../pom.xml</relativePath>', 1), ('  </parent>', 1), ('  <groupId>org.apache.spark</groupId>', 1), ('  <artifactId>spark-examples_2.10</artifactId>', 1), ('  <name>Spark Project Examples</name>', 1), ('  <url>http://spark.apache.org/</url>', 1), ('<groupId>org.apache.spark</groupId>', 11), ('<artifactId>spark-core_${scala.binary.version}</artifactId>', 1), ('<scope>provided</scope>', 8), ('<artifactId>spark-mllib_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-twitter_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-mqtt_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-zeromq_${scala.binary.version}</artifactId>', 1), ('  <exclusion>', 34), (' <groupId>org.spark-project.protobuf</groupId>', 1), ('  </exclusion>', 34), ('</exclusions>', 5), ('<artifactId>spark-streaming-kafka_${scala.binary.version}</artifactId>', 1), ('<groupId>org.apache.hbase</groupId>', 7), ('<artifactId>hbase-testing-util</artifactId>', 1), ('<version>${hbase.version}</version>', 7), ('<scope>${hbase.deps.scope}</scope>', 6), (' <groupId>org.jruby</groupId>', 1), ('<artifactId>hbase-client</artifactId>', 1), ('  <groupId>io.netty</groupId>', 1), (' </exclusion>', 1), ('  </exclusions>', 1), ('<artifactId>hbase-server</artifactId>', 1), (' <groupId>org.apache.hadoop</groupId>', 7), (' <artifactId>hadoop-client</artifactId>', 1), (' <artifactId>hadoop-mapreduce-client-jobclient</artifactId>', 1), (' <artifactId>hadoop-auth</artifactId>', 1), (' <artifactId>hadoop-hdfs</artifactId>', 1), (' <artifactId>hbase-hadoop1-compat</artifactId>', 1), (' <groupId>org.apache.commons</groupId>', 2), (' <artifactId>jersey-server</artifactId>', 1), (' <artifactId>jersey-json</artifactId>', 1), (' <!-- hbase uses v2.4, which is better, but ...-->', 1), (' <groupId>commons-io</groupId>', 1), ('<artifactId>hbase-hadoop-compat</artifactId>', 2), ('<type>test-jar</type>', 1), ('<groupId>org.apache.commons</groupId>', 1), ('<artifactId>algebird-core_${scala.binary.version}</artifactId>', 1), ('<version>0.9.0</version>', 1), ('<artifactId>scalacheck_${scala.binary.version}</artifactId>', 1), ('<groupId>org.apache.cassandra</groupId>', 1), (' <groupId>com.google.guava</groupId>', 1), (' <artifactId>guava</artifactId>', 1), (' <artifactId>concurrentlinkedhashmap-lru</artifactId>', 1), (' <groupId>com.ning</groupId>', 1), (' <artifactId>compress-lzf</artifactId>', 1), (' <groupId>commons-cli</groupId>', 1), (' <artifactId>commons-codec</artifactId>', 1), (' <groupId>io.netty</groupId>', 1), (' <artifactId>jline</artifactId>', 1), (' <groupId>net.jpountz.lz4</groupId>', 1), (' <artifactId>lz4</artifactId>', 1), (' <artifactId>commons-math3</artifactId>', 1), (' <groupId>org.apache.thrift</groupId>', 1), (' <artifactId>libthrift</artifactId>', 1), (' <!--', 1), ('The following dependencies are already present in the Spark assembly, so we want to force', 1), (' -->', 1), ('<groupId>org.scala-lang</groupId>', 1), ('<artifactId>scala-library</artifactId>', 1), (' <plugins>', 1), ('  <configuration>', 3), ('  <artifactId>maven-install-plugin</artifactId>', 1), ('  <include>*:*</include>', 1), ('</includes>', 1), (' <filters>', 1), ('  <excludes>', 1), (' <exclude>META-INF/*.SF</exclude>', 1), (' <transformers>', 1), ('<transformer implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">', 1), ('  <resource>reference.conf</resource>', 1), (' </transformers>', 1), (' </plugins>', 1), ('  <profiles>', 1), (' <profile>', 6), ('<id>kinesis-asl</id>', 1), ('  <dependency>', 1), (' <version>${project.version}</version>', 1), ('  </dependency>', 1), (' <!-- Profiles that disable inclusion of certain dependencies. -->', 1), ('<id>flume-provided</id>', 1), ('<properties>', 5), ('</properties>', 5), ('<id>hadoop-provided</id>', 1), ('<id>hive-provided</id>', 1), ('  <hive.deps.scope>provided</hive.deps.scope>', 1), ('</project>', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pom Count\\n\")\n",
    "print(pomCount.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "accurate-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join function combines the two datasets (K,V) and (K,W) together and get (K, (V,W))\n",
    "joined = readmeCount.join(pomCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eligible-voltage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', (43, 841))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the value to the console\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "soviet-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the values together to get the total count\n",
    "joinedSum = joined.map(lambda k: (k[0], (k[1][0]+k[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aging-monkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 884)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedSum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "incorrect-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Individial\n",
      "\n",
      "[('', (43, 841))]\n",
      "\n",
      "\n",
      "Joined Sum\n",
      "\n",
      "[('', 884)]\n"
     ]
    }
   ],
   "source": [
    "#print the first five elements from the joined and the joinedSum RDD\n",
    "print(\"Joined Individial\\n\")\n",
    "print(joined.take(5))\n",
    "\n",
    "print(\"\\n\\nJoined Sum\\n\")\n",
    "print(joinedSum.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-taylor",
   "metadata": {},
   "source": [
    "#### Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "mounted-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "functioning-principle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "approved-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "distinguished-profile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "infectious-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallelize an array of four integers and run it through a loop to add each integer value to the accumulator variable\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "def f(x):\n",
    "    global accum\n",
    "    accum += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "decent-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through each element of the rdd and apply the function f\n",
    "rdd.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "strategic-hindu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-genius",
   "metadata": {},
   "source": [
    "#### Key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "derived-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = ('a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "stopped-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print(pair[0])\n",
    "print(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-smith",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
